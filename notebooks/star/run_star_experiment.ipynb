{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380e1ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.data.STAR.star import STARDataset\n",
    "\n",
    "if not os.path.exists('star_results'):\n",
    "    os.makedirs('star_results')\n",
    "\n",
    "path_star_dataset = \"../../src/data/STAR/STAR_Students.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15c74b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (\n",
    "    LogisticRegressionCV,\n",
    "    RidgeCV,\n",
    ")\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from src.randomization_aware.combine_cate import CATECombiner\n",
    "from src.randomization_aware.learners import (\n",
    "    DRLearner,\n",
    "    QuasiOptimizedLearner,\n",
    ")\n",
    "from src.baselines.asaiee import AsaieeCATE\n",
    "from src.baselines.ksp import KSPCATE\n",
    "from src.baselines.pooling import TLearnerPooling\n",
    "from src.baselines.trial_only import TrialCATE\n",
    "from src.baselines.predict_ate import DifferenceInMeans\n",
    "from econml.metalearners import TLearner\n",
    "from econml.dr import DRLearner as econml_DRLearner\n",
    "\n",
    "crossfit_folds = 2\n",
    "\n",
    "regressor_outcome = HistGradientBoostingRegressor\n",
    "alphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "regressor_cate = lambda: RidgeCV(alphas=alphas)\n",
    "study_classifier = lambda: LogisticRegressionCV(\n",
    "    max_iter=1000, Cs=[1 / a for a in alphas], cv=3, solver=\"liblinear\"\n",
    ")\n",
    "\n",
    "cate_estimator_tlearner = lambda: TLearner(models=regressor_outcome())\n",
    "cate_estimator_dr = lambda: econml_DRLearner(\n",
    "    model_propensity=study_classifier(),\n",
    "    model_regression=regressor_outcome(),\n",
    "    model_final=regressor_cate(),\n",
    "    cv=crossfit_folds,\n",
    ")\n",
    "\n",
    "\n",
    "def get_drlearner_star(propensity_score):\n",
    "    return DRLearner(\n",
    "        propensity_score=propensity_score,\n",
    "        regressor_cate=regressor_cate(),\n",
    "        regressor_control=regressor_outcome(),\n",
    "        regressor_treated=regressor_outcome(),\n",
    "        crossfit_folds=crossfit_folds,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_quasioptimized_star(propensity_score):\n",
    "    return QuasiOptimizedLearner(\n",
    "        propensity_score=propensity_score,\n",
    "        regressor_cate=regressor_cate(),\n",
    "        regressor_control=regressor_outcome(),\n",
    "        regressor_treated=regressor_outcome(),\n",
    "        study_classifier=study_classifier(),\n",
    "        crossfit_folds=crossfit_folds,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_quasioptimized_star_unweighted(propensity_score):\n",
    "    return QuasiOptimizedLearner(\n",
    "        propensity_score=propensity_score,\n",
    "        regressor_cate=regressor_cate(),\n",
    "        regressor_control=regressor_outcome(),\n",
    "        regressor_treated=regressor_outcome(),\n",
    "        study_classifier=study_classifier(),\n",
    "        crossfit_folds=crossfit_folds,\n",
    "        remove_study_weighting=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_combined_star(propensity_score):\n",
    "    return CATECombiner(\n",
    "        propensity_score=propensity_score,\n",
    "        cate_learner_1=get_drlearner_star(propensity_score),\n",
    "        cate_learner_2=get_quasioptimized_star(propensity_score),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_asaiee_star(propensity_score):\n",
    "    return AsaieeCATE(\n",
    "        propensity_score=propensity_score,\n",
    "        regressor_cate=regressor_cate(),\n",
    "        regressor_control=regressor_outcome(),\n",
    "        regressor_treated=regressor_outcome(),\n",
    "        crossfit_folds=crossfit_folds,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_ksp_star(propensity_score):\n",
    "    return KSPCATE(\n",
    "        propensity_score,\n",
    "        cate_estimator=cate_estimator_tlearner(),\n",
    "        bias_correction_model=regressor_outcome(),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_pooling_star(propensity_score=None):\n",
    "    return TLearnerPooling(\n",
    "        regressor_control=regressor_outcome(),\n",
    "        regressor_treated=regressor_outcome(),\n",
    "        study_classifier=study_classifier(),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_tlearner_star(propensity_score=None):\n",
    "    return TrialCATE(cate_estimator=cate_estimator_tlearner())\n",
    "\n",
    "\n",
    "def get_dm(propensity_score=None):\n",
    "    return DifferenceInMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dcb423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for n1=1131: Cannot take a larger sample than population when 'replace=False'\n",
      "Error for n0=2221: Cannot take a larger sample than population when 'replace=False'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1121, 2211)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max_sample_sizes_independent(rct_fraction_of_rural, eval_fraction_of_rct):\n",
    "    dgp = STARDataset(path_star_dataset)\n",
    "    \n",
    "    max_n1 = 0\n",
    "    for n1 in range(1, 10000, 10):  # Increment n1 in steps of 100\n",
    "        try:\n",
    "            dgp.sample(n1, 1, rct_fraction_of_rural=rct_fraction_of_rural, eval_fraction_of_rct=eval_fraction_of_rct)\n",
    "            max_n1 = n1\n",
    "        except Exception as e:\n",
    "            print(f\"Error for n1={n1}: {e}\")\n",
    "            break  # Stop increasing n1 if an error occurs\n",
    "\n",
    "    max_n0 = 0\n",
    "    for n0 in range(1, 10000, 10):  # Increment n0 in steps of 100\n",
    "        try:\n",
    "            dgp.sample(1, n0, rct_fraction_of_rural=rct_fraction_of_rural, eval_fraction_of_rct=eval_fraction_of_rct)\n",
    "            max_n0 = n0\n",
    "        except Exception as e:\n",
    "            print(f\"Error for n0={n0}: {e}\")\n",
    "            break  # Stop increasing n0 if an error occurs\n",
    "\n",
    "    return max_n1, max_n0\n",
    "    \n",
    "\n",
    "rct_fraction_of_rural=0.5\n",
    "eval_fraction_of_rct=0.2\n",
    "get_max_sample_sizes_independent(rct_fraction_of_rural, eval_fraction_of_rct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d29a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 20250519_104710\n",
      "dropped_covar = g1surban, n1 = 1000; n0 = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:22<00:00, 22.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped_covar = g1surban, n1 = 1000; n0 = 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:23<00:00, 23.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped_covar = g1surban, n1 = 1000; n0 = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:23<00:00, 23.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped_covar = g1surban, n1 = 1000; n0 = 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped_covar = g1surban, n1 = 1000; n0 = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped_covar = g1surban, n1 = 1000; n0 = 1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:28<00:00, 28.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped_covar = g1surban, n1 = 1000; n0 = 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:22<00:00, 22.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped_covar = g1surban, n1 = 1000; n0 = 1750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:30<00:00, 30.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped_covar = g1surban, n1 = 1000; n0 = 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:29<00:00, 29.98s/it]\n"
     ]
    }
   ],
   "source": [
    "methods = {\n",
    "    \"DR-learner\": get_drlearner_star,\n",
    "    \"QR-learner\": get_quasioptimized_star,\n",
    "    # \"QR-learner (unweighted)\" : get_quasioptimized_star_unweighted,\n",
    "    \"Combined QR- and DR-learner\": get_combined_star,\n",
    "    \"Asaiee et al. [2023]\": get_asaiee_star,\n",
    "    \"Kallus et al. [2018]\": get_ksp_star,\n",
    "    \"Pooled T-learner\": get_pooling_star,\n",
    "    \"T-Learner\": get_tlearner_star,\n",
    "    \"Predict ATE\": get_dm,\n",
    "}\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"timestamp: {timestamp}\")\n",
    "iterations = 1\n",
    "n1_list = [1000]\n",
    "n0_list = [100, 250, 500, 750, 1000, 1250, 1500, 1750, 2000]\n",
    "fraction_rural_list = [0.5]\n",
    "covar_to_drop = [\n",
    "    \"g1surban\",\n",
    "]\n",
    "rows = []\n",
    "\n",
    "full_covar_list = [\n",
    "    \"g1surban\",\n",
    "    \"gender\",\n",
    "    \"race\",\n",
    "    \"birthmonth\",\n",
    "    \"birthday\",\n",
    "    \"birthyear\",\n",
    "    \"gkfreelunch\",\n",
    "    \"g1tchid\",\n",
    "    \"g1freelunch\",\n",
    "]\n",
    "\n",
    "for dropped_covar in covar_to_drop:\n",
    "    if dropped_covar is None:\n",
    "        covar_list = full_covar_list\n",
    "    else:\n",
    "        if isinstance(dropped_covar, list):\n",
    "            if all(covar in full_covar_list for covar in dropped_covar):\n",
    "                covar_list = [\n",
    "                    covar for covar in full_covar_list if covar not in dropped_covar\n",
    "                ]\n",
    "            else:\n",
    "                missing = [\n",
    "                    covar for covar in dropped_covar if covar not in full_covar_list\n",
    "                ]\n",
    "                raise ValueError(f\"{missing} not found in full_covar_list\")\n",
    "        elif dropped_covar in full_covar_list:\n",
    "            covar_list = [covar for covar in full_covar_list if covar != dropped_covar]\n",
    "        else:\n",
    "            raise ValueError(f\"{dropped_covar} not found in full_covar_list\")\n",
    "    dgp = STARDataset(path_star_dataset, cat_covar_columns=covar_list)\n",
    "    propensity_score_rct = dgp.get_propensity_score()\n",
    "    for fraction_rural in fraction_rural_list:\n",
    "        for n1 in n1_list:\n",
    "            for n0 in n0_list:\n",
    "                print(f\"dropped_covar = {dropped_covar}, n1 = {n1}; n0 = {n0}\")\n",
    "                for i in tqdm(range(iterations)):\n",
    "\n",
    "                    X_train, S_train, A_train, Y_train, X_eval, gt_adjusted_ite_eval = (\n",
    "                        dgp.sample(\n",
    "                            n1,\n",
    "                            n0,\n",
    "                            rct_fraction_of_rural=fraction_rural,\n",
    "                            eval_fraction_of_rct=eval_fraction_of_rct,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    for method_name, method_func in methods.items():\n",
    "\n",
    "                        estimator = method_func(propensity_score_rct)\n",
    "\n",
    "                        try:\n",
    "                            estimator.fit(X_train, S_train, A_train, Y_train)\n",
    "                            predictions = estimator.predict(X_eval)\n",
    "\n",
    "                            assert predictions.shape == gt_adjusted_ite_eval.shape\n",
    "                            rmse = np.sqrt(\n",
    "                                np.mean((gt_adjusted_ite_eval - predictions) ** 2)\n",
    "                            )\n",
    "                            rows.append(\n",
    "                                {\n",
    "                                    \"i\": i,\n",
    "                                    \"n1\": n1,\n",
    "                                    \"n0\": n0,\n",
    "                                    \"dropped_covar\": (\n",
    "                                        dropped_covar\n",
    "                                        if dropped_covar is not None\n",
    "                                        else \"None dropped\"\n",
    "                                    ),\n",
    "                                    \"fraction_rural\": fraction_rural,\n",
    "                                    \"method\": method_name,\n",
    "                                    \"rmse\": rmse\n",
    "                                }\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            rows.append(\n",
    "                                {\n",
    "                                    \"i\": i,\n",
    "                                    \"n1\": n1,\n",
    "                                    \"n0\": n0,\n",
    "                                    \"dropped_covar\": (\n",
    "                                        dropped_covar\n",
    "                                        if dropped_covar is not None\n",
    "                                        else \"None dropped\"\n",
    "                                    ),\n",
    "                                    \"method\": method_name,\n",
    "                                    \"error\": str(e),\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                        # Save results in a CSV file with a timestamp\n",
    "                        results_df = pd.DataFrame(rows)\n",
    "                        results_df.to_csv(\n",
    "                            f\"star_results/experiment_{timestamp}.csv\", index=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1babb861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:22<00:00,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejection rate of null for treated (transportability holds): 0.998\n",
      "Rejection rate of null for control (transportability holds): 0.073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "dgp = STARDataset(\n",
    "    path_star_dataset,\n",
    "    cat_covar_columns=[\n",
    "        \"g1surban\",\n",
    "        \"gender\",\n",
    "        \"race\",\n",
    "        \"birthmonth\",\n",
    "        \"birthday\",\n",
    "        \"birthyear\",\n",
    "        \"gkfreelunch\",\n",
    "        \"g1tchid\",\n",
    "        \"g1freelunch\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "iterations = 1000\n",
    "reject_treated_list = []\n",
    "reject_control_list = []\n",
    "\n",
    "for i in tqdm(range(iterations)):\n",
    "\n",
    "\n",
    "    X_train, S_train, A_train, Y_train, X_eval, gt_adjusted_ite_eval = dgp.sample(\n",
    "        1000,\n",
    "        1000,\n",
    "        rct_fraction_of_rural=0.5,\n",
    "        eval_fraction_of_rct=0.2,\n",
    "    )\n",
    "\n",
    "    # Standardize X_train\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Fit a linear regression model for S_train on X_train\n",
    "    reg = LinearRegression().fit(X_train_scaled, S_train)\n",
    "\n",
    "    # Separate data based on A_train\n",
    "    mask_treated = A_train.flatten() == 1\n",
    "    mask_control = A_train.flatten() == 0\n",
    "\n",
    "    # Get residuals of S_train for treated and control groups\n",
    "    S_train_residuals_treated = S_train[mask_treated] - reg.predict(X_train_scaled[mask_treated])\n",
    "    S_train_residuals_control = S_train[mask_control] - reg.predict(X_train_scaled[mask_control])\n",
    "\n",
    "    # Fit a linear regression model for Y_train on X_train for treated and control groups\n",
    "    reg_treated = LinearRegression().fit(X_train_scaled[mask_treated], Y_train[mask_treated])\n",
    "    reg_control = LinearRegression().fit(X_train_scaled[mask_control], Y_train[mask_control])\n",
    "\n",
    "    # Get residuals of Y_train for treated and control groups\n",
    "    Y_train_residuals_treated = Y_train[mask_treated] - reg_treated.predict(X_train_scaled[mask_treated])\n",
    "    Y_train_residuals_control = Y_train[mask_control] - reg_control.predict(X_train_scaled[mask_control])\n",
    "\n",
    "    # Compute the partial correlation for treated and control groups\n",
    "    corr_treated, p_value_treated = pearsonr(\n",
    "        Y_train_residuals_treated.flatten(), S_train_residuals_treated.flatten()\n",
    "    )\n",
    "    corr_control, p_value_control = pearsonr(\n",
    "        Y_train_residuals_control.flatten(), S_train_residuals_control.flatten()\n",
    "    )\n",
    "\n",
    "    # Count number of rejections\n",
    "    reject_treated = p_value_treated < 0.05\n",
    "    reject_control = p_value_control < 0.05\n",
    "\n",
    "    reject_treated_list.append(reject_treated)\n",
    "    reject_control_list.append(reject_control)\n",
    "\n",
    "print(\"Rejection rate of null for treated (transportability holds):\", np.mean(reject_treated_list))\n",
    "print(\"Rejection rate of null for control (transportability holds):\", np.mean(reject_control_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
